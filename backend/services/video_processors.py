"""
å‹•ç”»å‡¦ç†å°‚ç”¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰

MediaPipe ã‚’ä½¿ã£ãŸéª¨æ ¼æ¤œå‡ºãƒ»å§¿å‹¢åˆ†æ
"""
import asyncio
import logging
import numpy as np
from typing import Dict, Any, Optional
import cv2

logger = logging.getLogger(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
_pose_detector = None
_face_mesh_detector = None


class PoseAnalyzer:
    """å§¿å‹¢åˆ†æãƒ—ãƒ­ã‚»ãƒƒã‚µï¼ˆMediaPipe Poseï¼‰"""
    
    def __init__(self):
        self.pose = None
        
    async def initialize(self):
        """MediaPipe Pose ã‚’åˆæœŸåŒ–"""
        global _pose_detector
        if _pose_detector is None:
            try:
                import mediapipe as mp
                logger.info("MediaPipe Pose ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                _pose_detector = mp.solutions.pose.Pose(
                    static_image_mode=False,
                    model_complexity=1,
                    smooth_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
                logger.info("MediaPipe Pose ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            except Exception as e:
                logger.warning(f"MediaPipe Pose ã®ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                _pose_detector = None
        self.pose = _pose_detector
        
    async def analyze(self, frame_data: bytes) -> Dict[str, Any]:
        """
        æ˜ åƒãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å§¿å‹¢ã‚’åˆ†æ
        
        Args:
            frame_data: JPEGç”»åƒãƒã‚¤ãƒŠãƒª
            
        Returns:
            å§¿å‹¢ã‚¹ã‚³ã‚¢ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        """
        if self.pose is None:
            await self.initialize()
            if self.pose is None:
                return self._default_posture()
        
        try:
            # JPEG â†’ numpy array
            nparr = np.frombuffer(frame_data, np.uint8)
            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            
            if image is None:
                return self._default_posture()
            
            # RGBå¤‰æ›
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # MediaPipe ã§å‡¦ç†
            results = await asyncio.to_thread(self.pose.process, image_rgb)
            
            if not results.pose_landmarks:
                return {
                    "score": 50,
                    "feedback": "å§¿å‹¢ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ",
                    "details": {}
                }
            
            # éª¨æ ¼ã‹ã‚‰å§¿å‹¢ã‚’è©•ä¾¡
            landmarks = results.pose_landmarks.landmark
            
            # è‚©ã®æ°´å¹³åº¦ãƒã‚§ãƒƒã‚¯
            left_shoulder = landmarks[11]
            right_shoulder = landmarks[12]
            shoulder_angle = abs(left_shoulder.y - right_shoulder.y)
            
            # èƒŒç­‹ã®çœŸã£ã™ãã•ãƒã‚§ãƒƒã‚¯
            nose = landmarks[0]
            left_hip = landmarks[23]
            spine_alignment = abs(nose.x - left_hip.x)
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            score = 100
            feedback_parts = []
            
            if shoulder_angle > 0.05:
                score -= 20
                feedback_parts.append("è‚©ãŒå‚¾ã„ã¦ã„ã¾ã™")
            
            if spine_alignment > 0.1:
                score -= 15
                feedback_parts.append("èƒŒç­‹ã‚’ä¼¸ã°ã—ã¾ã—ã‚‡ã†")
            
            # é¡”ã®ä½ç½®ãƒã‚§ãƒƒã‚¯ï¼ˆå‰å‚¾ãƒ»å¾Œå‚¾ï¼‰
            if nose.z > 0:
                score -= 10
                feedback_parts.append("ã‚„ã‚„å‰å‚¾å§¿å‹¢ã§ã™")
            
            score = max(0, min(100, score))
            
            if score >= 80:
                feedback = "âœ¨ å§¿å‹¢ãŒè‰¯å¥½ã§ã™ï¼"
            elif score >= 60:
                feedback = "ğŸ‘ ã¾ãšã¾ãšã®å§¿å‹¢ã§ã™ã€‚" + ("ã€".join(feedback_parts) if feedback_parts else "")
            else:
                feedback = "ğŸ’¡ å§¿å‹¢ã‚’æ”¹å–„ã—ã¾ã—ã‚‡ã†: " + "ã€".join(feedback_parts)
            
            return {
                "score": int(score),
                "feedback": feedback,
                "details": {
                    "shoulder_level": float(1 - shoulder_angle * 10),
                    "spine_alignment": float(1 - spine_alignment * 5),
                    "forward_lean": float(-nose.z) if nose.z < 0 else 0.0
                }
            }
            
        except Exception as e:
            logger.warning(f"å§¿å‹¢åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return self._default_posture()
    
    def _default_posture(self) -> Dict[str, Any]:
        return {
            "score": 75,
            "feedback": "å§¿å‹¢åˆ†æä¸­...",
            "details": {}
        }


class EyeContactAnalyzer:
    """è¦–ç·šåˆ†æãƒ—ãƒ­ã‚»ãƒƒã‚µï¼ˆMediaPipe Face Meshï¼‰"""
    
    def __init__(self):
        self.face_mesh = None
        
    async def initialize(self):
        """MediaPipe Face Mesh ã‚’åˆæœŸåŒ–"""
        global _face_mesh_detector
        if _face_mesh_detector is None:
            try:
                import mediapipe as mp
                logger.info("MediaPipe Face Mesh ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                _face_mesh_detector = mp.solutions.face_mesh.FaceMesh(
                    static_image_mode=False,
                    max_num_faces=1,
                    refine_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
                logger.info("MediaPipe Face Mesh ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            except Exception as e:
                logger.warning(f"MediaPipe Face Mesh ã®ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                _face_mesh_detector = None
        self.face_mesh = _face_mesh_detector
        
    async def analyze(self, frame_data: bytes) -> Dict[str, Any]:
        """
        æ˜ åƒãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰è¦–ç·šã‚’åˆ†æ
        
        Args:
            frame_data: JPEGç”»åƒãƒã‚¤ãƒŠãƒª
            
        Returns:
            è¦–ç·šã‚¹ã‚³ã‚¢ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        """
        if self.face_mesh is None:
            await self.initialize()
            if self.face_mesh is None:
                return self._default_eye_contact()
        
        try:
            # JPEG â†’ numpy array
            nparr = np.frombuffer(frame_data, np.uint8)
            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            
            if image is None:
                return self._default_eye_contact()
            
            # RGBå¤‰æ›
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # MediaPipe ã§å‡¦ç†
            results = await asyncio.to_thread(self.face_mesh.process, image_rgb)
            
            if not results.multi_face_landmarks:
                return {
                    "score": 50,
                    "feedback": "é¡”ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ",
                    "details": {}
                }
            
            # è™¹å½©ã®ä½ç½®ã‹ã‚‰è¦–ç·šæ–¹å‘ã‚’æ¨å®š
            landmarks = results.multi_face_landmarks[0].landmark
            
            # å·¦ç›®ãƒ»å³ç›®ã®è™¹å½©ä½ç½®
            left_iris = landmarks[468]  # MediaPipe ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            right_iris = landmarks[473]
            
            # ç›®ã®ä¸­å¿ƒã‹ã‚‰ã®ã‚ºãƒ¬ã‚’è¨ˆç®—
            left_eye_center = landmarks[33]
            right_eye_center = landmarks[263]
            
            left_offset = abs(left_iris.x - left_eye_center.x)
            right_offset = abs(right_iris.x - right_eye_center.x)
            
            avg_offset = (left_offset + right_offset) / 2
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆã‚«ãƒ¡ãƒ©ã‚’è¦‹ã¦ã„ã‚‹ã»ã©é«˜ã„ï¼‰
            score = max(0, min(100, int((1 - avg_offset * 5) * 100)))
            
            if score >= 80:
                feedback = "ğŸ‘ï¸ ã‚«ãƒ¡ãƒ©ã‚’ã—ã£ã‹ã‚Šè¦‹ã¦ã„ã¾ã™"
            elif score >= 60:
                feedback = "ğŸ‘€ ã»ã¼ã‚«ãƒ¡ãƒ©ã‚’è¦‹ã¦ã„ã¾ã™"
            else:
                feedback = "ğŸ’¡ ã‚«ãƒ¡ãƒ©ã‚’è¦‹ã‚‹ã‚ˆã†æ„è­˜ã—ã¾ã—ã‚‡ã†"
            
            return {
                "score": score,
                "feedback": feedback,
                "details": {
                    "gaze_offset": float(avg_offset),
                    "looking_away": avg_offset > 0.2
                }
            }
            
        except Exception as e:
            logger.warning(f"è¦–ç·šåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return self._default_eye_contact()
    
    def _default_eye_contact(self) -> Dict[str, Any]:
        return {
            "score": 70,
            "feedback": "è¦–ç·šåˆ†æä¸­...",
            "details": {}
        }
